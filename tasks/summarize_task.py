from crewai import Task
from agents.summarizer_agent import summarizer_agent
from openai import OpenAI
import math

class SummarizeTask(Task):
    # O __init__ volta a ser simples, sem argumentos
    def __init__(self):
        super().__init__(
            description=(
                "Analisar o seguinte texto, que foi extraÃ­do de um documento, "
                "e gerar um resumo conciso e bem estruturado. Texto: {document_content}"
            ),
            expected_output="Um resumo textual coeso do documento de entrada.",
            agent=summarizer_agent,
        )

    # O mÃ©todo run agora pega o texto do dicionÃ¡rio de contexto
    def run(self, context: dict):
        # O 'context' aqui Ã© o dicionÃ¡rio `inputs` que passamos no kickoff
        document_text = context['document_content']

        if not document_text or not document_text.strip():
            print("âš ï¸ Texto de entrada estÃ¡ vazio.")
            return "O texto de entrada estava vazio."

        client = OpenAI(api_key="ollama", base_url="http://localhost:11434/v1")
        
        chunk_size = 4000
        chunks = [document_text[i:i + chunk_size] for i in range(0, len(document_text), chunk_size)]
        
        # ... O resto do seu cÃ³digo de chunking continua exatamente o mesmo daqui para baixo ...
        print(f"ðŸ¤– Texto muito longo. Dividindo em {len(chunks)} pedaÃ§os para resumir...")
        
        partial_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"ðŸ“„ Resumindo pedaÃ§o {i + 1}/{len(chunks)}...")
            try:
                messages = [
                    {"role": "system", "content": "VocÃª Ã© um especialista em sumarizar partes de um documento. Sua tarefa Ã© criar um resumo conciso do texto a seguir, focando nos pontos principais. Este Ã© um pedaÃ§o de um documento maior."},
                    {"role": "user", "content": f"Por favor, resuma o seguinte trecho:\n\n{chunk}"}
                ]
                
                resposta = client.chat.completions.create(
                    model="ollama/llama3.1:8b", 
                    messages=messages,
                    temperature=0.0,
                )
                summary_part = resposta.choices[0].message.content
                partial_summaries.append(summary_part)
                
            except Exception as e:
                print(f"âš ï¸ Erro ao resumir o pedaÃ§o {i + 1}: {e}")
                partial_summaries.append(f"[Erro ao processar o pedaÃ§o {i+1}]")

        print("âœ… Todos os pedaÃ§os foram resumidos. Criando o resumo final...")
        
        final_summary_text = "\n\n".join(partial_summaries)
        
        if len(final_summary_text) > chunk_size:
             print("ðŸ“„ O conjunto de resumos ainda Ã© grande, criando um resumo final consolidado...")
             messages = [
                {"role": "system", "content": "VocÃª Ã© um especialista em consolidar informaÃ§Ãµes. A seguir estÃ£o vÃ¡rios resumos parciais de um mesmo documento. Sua tarefa Ã© criar um Ãºnico resumo final coeso e bem estruturado a partir deles."},
                {"role": "user", "content": f"Por favor, crie um resumo final a partir dos seguintes pontos:\n\n{final_summary_text}"}
            ]
             resposta_final = client.chat.completions.create(
                    model="ollama/llama3.1:8b", 
                    messages=messages,
                    temperature=0.2,
                )
             final_summary = resposta_final.choices[0].message.content
        else:
            final_summary = final_summary_text

        print("âœ… Resumo final gerado com sucesso.")
        return final_summary